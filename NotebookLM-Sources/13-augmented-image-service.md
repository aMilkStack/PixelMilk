/**
 * Augmented Image Service
 * Generates infographics and analyzes image regions
 * Reference implementation from AI Studio apps
 */

import { GoogleGenAI } from "@google/genai";
import { AnalysisResult, GeneratedImage } from "../types";

export const generateInfographic = async (query: string): Promise<GeneratedImage> => {
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const prompt = `
Create an explanation-driven, sparse-text, rich image about: "${query}"

IMPORTANT STYLE GUIDELINES:
- Create a diagram or infographic but with minimal text - keep it visually focused
- Focus on compelling imagery, scenes, objects, or artistic representations
- Use dramatic lighting, rich colors, and cinematic composition
- Think editorial photography or concept art, not charts or diagrams
- The image should be atmospheric and immersive

Generate a stunning visual that captures the essence of the topic through imagery, not words.`;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: prompt,
      config: {
        imageConfig: {
          aspectRatio: '16:9',
        },
      },
    });

    // Extract Image
    let imageBase64: string | undefined;
    let mimeType = 'image/png';

    // The response structure for images in gemini-2.5-flash-image contains multiple parts
    const parts = response.candidates?.[0]?.content?.parts;

    if (parts) {
      for (const part of parts) {
        if (part.inlineData) {
            imageBase64 = part.inlineData.data;
            mimeType = part.inlineData.mimeType || 'image/png';
            break;
        }
      }
    }

    if (!imageBase64) {
      throw new Error("No image generated by the model.");
    }

    // Extract Grounding Metadata (Likely empty for flash-image without tools, but kept for safety)
    const groundingUrls: Array<{ title: string; uri: string }> = [];
    // flash-image doesn't support tools currently in this config, so this will likely be empty
    const chunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks;
    if (chunks) {
        chunks.forEach(chunk => {
            if (chunk.web) {
                groundingUrls.push({ title: chunk.web.title || 'Source', uri: chunk.web.uri || '#' });
            }
        });
    }

    return {
      base64: imageBase64,
      mimeType,
      groundingUrls
    };

  } catch (error) {
    console.error("Image Generation Error:", error);
    throw error;
  }
};

export const analyzeImageRegions = async (query: string, imageBase64: string): Promise<AnalysisResult> => {
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const prompt = `
Analyze this image about "${query}" and identify interesting regions to annotate.
Use Google Search to verify facts, statistics, and find reputable source URLs.

Identify 4-6 distinct visual areas in the image. For each area, provide widget data.

CRITICAL CONTENT GUIDELINES:
- **RICH DESCRIPTIONS:** Do not write one-liners. Descriptions must be 2-3 sentences, immersive, and educational. Explain *why* this part matters.
- **ICONS:** "icon" must be a SINGLE valid Emoji that visually represents the segment. Do not leave blank.
- **TONE:** Scientific, futuristic, yet accessible.

For each segment, choose the BEST format from:
- "compact": Standard card (use for most items).
- "stats": Focus on numerical data points (use if you can invent or cite plausible metrics).
- "detailed": Long-form explanation for the central subject.

Provide this data:
- "label": Name (1-4 words)
- "format": "compact" | "stats" | "detailed"
- "description": Rich text (approx 30-50 words).
- "category": "concept" | "data" | "process" | "structure"
- "icon": A single relevant emoji (e.g. ðŸ§¬, âš¡, ðŸŒŒ).
- "stats": Array of facts (ONLY for stats/detailed formats) { "label", "value" }
- "sourceUrl": A relevant Wikipedia or educational URL found via search.
- "sourceName": Short name for the source.
- "bounds": { "x": number (0-100), "y": number (0-100), "width": number (0-100), "height": number (0-100) }

Mix formats. Ensure the "bounds" accurately target specific visual elements in the image.

Return as JSON:
{
  "segments": [ ... ]
}

Return ONLY valid JSON.`;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: [
        {
          role: 'user',
          parts: [
            { text: prompt },
            {
              inlineData: {
                mimeType: 'image/png',
                data: imageBase64,
              },
            },
          ],
        },
      ],
      config: {
          tools: [{ googleSearch: {} }]
      }
    });

    const text = response.text;
    if (!text) throw new Error("No analysis generated");

    // Basic cleanup just in case JSON mode misses something or adds markdown
    const cleanText = text.replace(/```json/g, '').replace(/```/g, '').trim();
    return JSON.parse(cleanText) as AnalysisResult;

  } catch (error) {
    console.error("Image Analysis Error:", error);
    throw error;
  }
};
